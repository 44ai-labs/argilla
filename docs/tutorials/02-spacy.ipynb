{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üí´  Explore and analyze `spaCy` NER pipelines\n",
    "\n",
    "In this tutorial, we will learn to log [spaCy](https://spacy.io/) Name Entity Recognition (NER) predictions. \n",
    "\n",
    "This is useful for: \n",
    "\n",
    "- üßêEvaluating pre-trained models.\n",
    "- üîéSpotting frequent errors both during development and production. \n",
    "- üìàImproving your pipelines over time using Rubrix annotation mode.\n",
    "- üéÆMonitoring your model predictions using Rubrix integration with Kibana\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "\n",
    "<video width=\"100%\" controls><source src=\"../_static/tutorials/02-spacy/spacy.mp4\" type=\"video/mp4\"></video>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial we will learn how to explore and analyze spaCy NER pipelines in an easy way.\n",
    "\n",
    "We will load the [*Gutenberg Time*](https://huggingface.co/datasets/gutenberg_time) dataset from the Hugging Face Hub and use a transformer-based spaCy model for detecting entities in this dataset and log the detected entities into a Rubrix dataset. This dataset can be used for exploring the quality of predictions and for creating a new training set, by correcting, adding and validating entities.\n",
    "\n",
    "Then, we will use a smaller spaCy model for detecting entities and log the detected entities into the same Rubrix dataset for comparing its predictions with the previous model. And, as a bonus, we will use Rubrix and spaCy on a more challenging dataset: IMDB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Rubrix is a free and open-source tool to explore, annotate, and monitor data for NLP projects. \n",
    "\n",
    "If you are new to Rubrix, visit and ‚≠ê star Rubrix for more materials like and detailed docs: [Github repo](https://github.com/recognai/rubrix)\n",
    "\n",
    "If you have not installed and launched Rubrix yet, check the [Setup and Installation guide](https://docs.rubrix.ml/en/latest/getting_started/setup%26installation.html).\n",
    "\n",
    "For this tutorial we also need the third party libraries datasets and of course spaCy together with pytorch, which can be installed via git:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "openapi-python-client 0.8.0 requires attrs<21.0.0,>=20.1.0, but you have attrs 21.2.0 which is incompatible.\r\n",
      "en-core-web-trf 3.2.0 requires spacy<3.3.0,>=3.2.0, but you have spacy 3.1.0 which is incompatible.\r\n",
      "en-core-web-trf 3.2.0 requires spacy-transformers<1.2.0,>=1.1.2, but you have spacy-transformers 1.0.6 which is incompatible.\r\n",
      "black 22.1.0 requires click>=8.0.0, but you have click 7.1.2 which is incompatible.\u001B[0m\u001B[31m\r\n",
      "\u001B[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch -qqq\n",
    "%pip install datasets \"spacy[transformers]~=3.0\" protobuf -qqq "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our dataset\n",
    "For this tutorial, we're going to use the [*Gutenberg Time*](https://huggingface.co/datasets/gutenberg_time) dataset from the Hugging Face Hub. It contains all explicit time references in a dataset of 52,183 novels whose full text is available via Project Gutenberg. From extracts of novels, we are surely going to find some NER entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"gutenberg_time\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the first 5 examples of the train set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "  guten_id hour_reference                  time_phrase  is_ambiguous  \\\n0     4447              5                 five o'clock          True   \n1     4447             12  the fall of the winter noon          True   \n2    28999             12                       midday          True   \n3    28999             12                       midday          True   \n4    28999              0                     midnight          True   \n\n   time_pos_start  time_pos_end  \\\n0             145           147   \n1              68            74   \n2              46            47   \n3             133           134   \n4              43            44   \n\n                                         tok_context  \n0  I crossed the ground she had traversed , notin...  \n1  So profoundly penetrated with thoughtfulness w...  \n2  And here is Hendon , and it is time for us to ...  \n3  Sorrows and trials she had had in plenty in he...  \n4  Jeannie joined her friend in the window-seat ....  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>guten_id</th>\n      <th>hour_reference</th>\n      <th>time_phrase</th>\n      <th>is_ambiguous</th>\n      <th>time_pos_start</th>\n      <th>time_pos_end</th>\n      <th>tok_context</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4447</td>\n      <td>5</td>\n      <td>five o'clock</td>\n      <td>True</td>\n      <td>145</td>\n      <td>147</td>\n      <td>I crossed the ground she had traversed , notin...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4447</td>\n      <td>12</td>\n      <td>the fall of the winter noon</td>\n      <td>True</td>\n      <td>68</td>\n      <td>74</td>\n      <td>So profoundly penetrated with thoughtfulness w...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>28999</td>\n      <td>12</td>\n      <td>midday</td>\n      <td>True</td>\n      <td>46</td>\n      <td>47</td>\n      <td>And here is Hendon , and it is time for us to ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>28999</td>\n      <td>12</td>\n      <td>midday</td>\n      <td>True</td>\n      <td>133</td>\n      <td>134</td>\n      <td>Sorrows and trials she had had in plenty in he...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>28999</td>\n      <td>0</td>\n      <td>midnight</td>\n      <td>True</td>\n      <td>43</td>\n      <td>44</td>\n      <td>Jeannie joined her friend in the window-seat ....</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(dataset.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging spaCy NER entities into Rubrix\n",
    "\n",
    "### Using a Transformer-based pipeline\n",
    "\n",
    "Let's download our Roberta-based pretrained pipeline and instantiate a spaCy `nlp` pipeline with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-trf==3.1.0\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.1.0/en_core_web_trf-3.1.0-py3-none-any.whl (460.2 MB)\r\n",
      "\u001B[2K     \u001B[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m \u001B[32m460.2/460.2 MB\u001B[0m \u001B[31m2.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: spacy<3.2.0,>=3.1.0 in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from en-core-web-trf==3.1.0) (3.1.0)\r\n",
      "Requirement already satisfied: spacy-transformers<1.1.0,>=1.0.3 in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from en-core-web-trf==3.1.0) (1.0.6)\r\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.7 in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (8.0.13)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (2.0.6)\r\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (0.6.1)\r\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (1.19.5)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (2.4.2)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (2.0.6)\r\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (0.8.2)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (2.26.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (21.0)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (1.0.6)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (58.2.0)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (3.0.8)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (2.11.3)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (0.7.5)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (4.62.3)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (1.8.2)\r\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (0.3.2)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (3.0.6)\r\n",
      "Requirement already satisfied: transformers<4.10.0,>=3.4.0 in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (4.5.1)\r\n",
      "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (0.8.4)\r\n",
      "Requirement already satisfied: torch>=1.5.0 in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (1.10.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (3.0.7)\r\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (5.2.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (3.10.0.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (2021.10.8)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (2.0.7)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (1.26.7)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (3.3)\r\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (0.10.3)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (2021.10.23)\r\n",
      "Requirement already satisfied: sacremoses in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (0.0.46)\r\n",
      "Requirement already satisfied: filelock in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (3.3.1)\r\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (7.1.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from jinja2->spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (2.0.1)\r\n",
      "Requirement already satisfied: six in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from sacremoses->transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (1.16.0)\r\n",
      "Requirement already satisfied: joblib in /usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages (from sacremoses->transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (1.1.0)\r\n",
      "Installing collected packages: en-core-web-trf\r\n",
      "  Attempting uninstall: en-core-web-trf\r\n",
      "    Found existing installation: en-core-web-trf 3.2.0\r\n",
      "    Uninstalling en-core-web-trf-3.2.0:\r\n",
      "      Successfully uninstalled en-core-web-trf-3.2.0\r\n",
      "Successfully installed en-core-web-trf-3.1.0\r\n",
      "\u001B[38;5;2m‚úî Download and installation successful\u001B[0m\r\n",
      "You can now load the package via spacy.load('en_core_web_trf')\r\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply the nlp pipeline to the first 50 examples in our dataset, collecting the **tokens** and **NER entities**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages/jose/backends/cryptography_backend.py:18: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes, int_to_bytes\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "11bb10ddb3784b77838780a949948000"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/rubrix/lib/python3.8/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    }
   ],
   "source": [
    "import rubrix as rb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "records = []\n",
    "\n",
    "for record in tqdm(list(dataset.take(50))):\n",
    "    # We only need the text of each instance\n",
    "    text = record[\"tok_context\"]\n",
    "    \n",
    "    # spaCy Doc creation\n",
    "    doc = nlp(text)    \n",
    "    \n",
    "    # Entity annotations\n",
    "    entities = [\n",
    "        (ent.label_, ent.start_char, ent.end_char)  \n",
    "        for ent in doc.ents\n",
    "    ] \n",
    "\n",
    "    # Pre-tokenized input text\n",
    "    tokens = [token.text for token in doc]\n",
    "    \n",
    "    # Rubrix TokenClassificationRecord list\n",
    "    records.append(\n",
    "        rb.TokenClassificationRecord(\n",
    "            text=text,\n",
    "            tokens=tokens,\n",
    "            prediction=entities,\n",
    "            prediction_agent=\"en_core_web_trf\",\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb.log(records=records, name=\"gutenberg_spacy_ner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you go to the `gutenberg_spacy_ner` dataset in Rubrix you can explore the predictions of this model.\n",
    "\n",
    "You can:\n",
    "\n",
    "- Filter records containing specific entity types,\n",
    "- See the most frequent \"mentions\" or surface forms for each entity. Mentions are the string values of specific entity types, such as for example \"1 month\" can be the mention of a duration entity. This is useful for error analysis, to quickly see potential issues and problematic entity types,\n",
    "- Use the free-text search to find records containing specific words, \n",
    "- And validate, include or reject specific entity annotations to build a new training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a smaller but more efficient pipeline\n",
    "\n",
    "Now let's compare with a smaller, but more efficient pre-trained model. \n",
    "\n",
    "Let's first download it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []    # Creating and empty record list to save all the records\n",
    "\n",
    "for record in tqdm(list(dataset.take(50))):\n",
    "\n",
    "    text = record[\"tok_context\"]  # We only need the text of each instance\n",
    "    doc = nlp(text)    # spaCy Doc creation\n",
    "    \n",
    "    # Entity annotations\n",
    "    entities = [\n",
    "        (ent.label_, ent.start_char, ent.end_char)  \n",
    "        for ent in doc.ents\n",
    "    ] \n",
    "\n",
    "    # Pre-tokenized input text\n",
    "    tokens = [token.text  for token in doc]\n",
    "    \n",
    "\n",
    "    # Rubrix TokenClassificationRecord list\n",
    "    records.append(\n",
    "        rb.TokenClassificationRecord(\n",
    "            text=text,\n",
    "            tokens=tokens,\n",
    "            prediction=entities,\n",
    "            prediction_agent=\"en_core_web_sm\",\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb.log(records=records, name=\"gutenberg_spacy_ner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring and comparing `en_core_web_sm` and `en_core_web_trf` models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you go to your `gutenberg_spacy_ner` dataset, you can explore and compare the results of both models. \n",
    "\n",
    "To only see predictions of a specific model, you can use the `predicted by` filter, which comes from the `prediction_agent` parameter of your `TextClassificationRecord`.\n",
    "\n",
    "\n",
    "![spacy_models_meta](../_static/tutorials/02-spacy/spacy_ner2.png \"spaCy models predicted_by filter\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the IMDB dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, both **spaCy pretrained models** seem to work pretty well. Let's try with a more challenging dataset, which is more dissimilar to the original training data these models have been trained on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = load_dataset(\"imdb\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for record in tqdm(imdb.select(range(50))):\n",
    "    # We only need the text of each instance\n",
    "    text = record[\"text\"]\n",
    "    \n",
    "    # spaCy Doc creation\n",
    "    doc = nlp(text)    \n",
    "    \n",
    "    # Entity annotations\n",
    "    entities = [\n",
    "        (ent.label_, ent.start_char, ent.end_char)  \n",
    "        for ent in doc.ents\n",
    "    ] \n",
    "\n",
    "    # Pre-tokenized input text\n",
    "    tokens = [token.text  for token in doc]\n",
    "    \n",
    "    # Rubrix TokenClassificationRecord list\n",
    "    records.append(\n",
    "        rb.TokenClassificationRecord(\n",
    "            text=text,\n",
    "            tokens=tokens,\n",
    "            prediction=entities,\n",
    "            prediction_agent=\"en_core_web_sm\",\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "\n",
    "rb.init(\"https://release-545c28052.rubrix.ml\", \"cd818cfa-899a-47e8-979d-6354221b3283\")\n",
    "# If you are using the default installation you can skip this line\n",
    "rb.set_workspace(\"deleteme\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "87b7640bfeee4f0aba337ae39d4b800b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 records logged to https://release-545c28052.rubrix.ml/ws/deleteme/imdb_spacy_ner\n"
     ]
    },
    {
     "data": {
      "text/plain": "BulkResponse(dataset='imdb_spacy_ner', processed=50, failed=0)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.log(records=records, name=\"imdb_spacy_ner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring this dataset highlights **the need of fine-tuning for specific domains**.\n",
    "\n",
    "For example, if we check the most frequent mentions for Person, we find two highly frequent missclassified entities: **gore** (the film genre) and **Oscar** (the prize). \n",
    "\n",
    "You can easily check every example by using the filters and search-box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this tutorial, you learned how to log and explore differnt `spaCy` NER models with Rubrix. Now you can:\n",
    "\n",
    "- Build custom dashboards using Kibana to monitor and visualize spaCy models.\n",
    "- Build training sets using pre-trained spaCy models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "### üìö [Rubrix documentation](https://docs.rubrix.ml) for more guides and tutorials.\n",
    "\n",
    "### üôã‚Äç‚ôÄÔ∏è Join the Rubrix community! A good place to start is the [discussion forum](https://github.com/recognai/rubrix/discussions).\n",
    "\n",
    "### ‚≠ê Rubrix [Github repo](https://github.com/recognai/rubrix) to stay updated."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b709380ea7d1cb2eb4650c0f11ac7e002ec6a534602815725771481b4784238c"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "0f338a8622467eba0ef87b9a79c52cc260cef0b0d60c3c739596fb787bf801dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
